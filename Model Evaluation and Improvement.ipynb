{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2cf34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install pandas matplotlib scipy scikit-learn seaborn ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf79274f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import clean data \n",
    "path = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DA0101EN-SkillsNetwork/labs/Data%20files/module_5_auto.csv'\n",
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836d72d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('module_5_auto.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f0b723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only use numeric data\n",
    "df=df._get_numeric_data()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e0cff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import  Libraries for plotting:\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43501913",
   "metadata": {},
   "source": [
    "### Functions for Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d950cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DistributionPlot(RedFunction, BlueFunction, RedName, BlueName, Title):\n",
    "    width = 12\n",
    "    height = 10\n",
    "    plt.figure(figsize=(width, height))\n",
    "\n",
    "    ax1 = sns.kdeplot(RedFunction, color=\"r\", label=RedName)\n",
    "    ax2 = sns.kdeplot(BlueFunction, color=\"b\", label=BlueName, ax=ax1)\n",
    "\n",
    "    plt.title(Title)\n",
    "    plt.xlabel('Price (in dollars)')\n",
    "    plt.ylabel('Proportion of Cars')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4536bc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PollyPlot(xtrain, xtest, y_train, y_test, lr,poly_transform):\n",
    "    width = 12\n",
    "    height = 10\n",
    "    plt.figure(figsize=(width, height))\n",
    "    \n",
    "    \n",
    "    #training data \n",
    "    #testing data \n",
    "    # lr:  linear regression object \n",
    "    #poly_transform:  polynomial transformation object \n",
    " \n",
    "    xmax=max([xtrain.values.max(), xtest.values.max()])\n",
    "\n",
    "    xmin=min([xtrain.values.min(), xtest.values.min()])\n",
    "\n",
    "    x=np.arange(xmin, xmax, 0.1)\n",
    "\n",
    "\n",
    "    plt.plot(xtrain, y_train, 'ro', label='Training Data')\n",
    "    plt.plot(xtest, y_test, 'go', label='Test Data')\n",
    "    plt.plot(x, lr.predict(poly_transform.fit_transform(x.reshape(-1, 1))), label='Predicted Function')\n",
    "    plt.ylim([-10000, 60000])\n",
    "    plt.ylabel('Price')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad85e21",
   "metadata": {},
   "source": [
    "### Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37ed6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to split my data into training and testing data. i will place the target data price in a separate dataframe y_data:\n",
    "y_data = df['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5d043a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop price data in dataframe x_data:\n",
    "x_data=df.drop('price',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c3cf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training and testing data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.10, random_state=1)\n",
    "\n",
    "\n",
    "print(\"number of test samples :\", x_test.shape[0])\n",
    "print(\"number of training samples:\",x_train.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ec7b0e",
   "metadata": {},
   "source": [
    "test_size = 0.10 means the testing set is 10% of the total dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e6fba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for 40% test size \n",
    "x_train1, x_test1, y_train1, y_test1 = train_test_split(x_data, y_data, test_size=0.40, random_state=1)\n",
    "\n",
    "\n",
    "print(\"number of test samples :\", x_test1.shape[0])\n",
    "print(\"number of training samples:\",x_train1.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24106dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7ca51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a Linear Regression object:\n",
    "lre=LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef1cb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We fit the model using the feature \"horsepower\":\n",
    "lre.fit(x_train[['horsepower']], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22345c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the R^2 on the test data:\n",
    "lre.score(x_test[['horsepower']], y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb58c7e",
   "metadata": {},
   "source": [
    "We can see the R^2 is much smaller using the test data compared to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9dd2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lre.score(x_train[['horsepower']], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bbe1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the R^2 on the test data using 40% of the dataset for testing\n",
    "lre.score(x_test1[['horsepower']], y_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b7f7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the R^2 on the test data using 40% of the dataset for tranning\n",
    "lre.score(x_train1[['horsepower']], y_train1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24b52f1",
   "metadata": {},
   "source": [
    "### Cross-Validation Score "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb470bcd",
   "metadata": {},
   "source": [
    "Sometimes testing data is not sufficient; as a result, we may want to perform cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b567f2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755c24d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We input the object, the feature (\"horsepower\"), and the target data (y_data). The parameter 'cv' determines the number of folds.\n",
    "Rcross = cross_val_score(lre, x_data[['horsepower']], y_data, cv=4)\n",
    "# The default scoring is R^2. Each element in the array has the average R^2 value for the fold:\n",
    "Rcross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67844908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the average and standard deviation of our estimate:\n",
    "print(\"The mean of the folds are\", Rcross.mean(), \"and the standard deviation is\" , Rcross.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05e9cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "-1 * cross_val_score(lre,x_data[['horsepower']], y_data,cv=4,scoring='neg_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd7a52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for 2 fold\n",
    "Rcross1 = cross_val_score(lre, x_data[['horsepower']], y_data, cv=2)\n",
    "\n",
    "print(\"The mean of the folds are\", Rcross1.mean(), \"and the standard deviation is\" , Rcross1.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4108f02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rcross1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067a8bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross predict\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "yhat = cross_val_predict(lre,x_data[['horsepower']], y_data,cv=4)\n",
    "yhat[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f469f69",
   "metadata": {},
   "source": [
    "### Overfitting, Underfitting and Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422f8b0d",
   "metadata": {},
   "source": [
    "Multiple Linear Regression objects and train the model using 'horsepower', 'curb-weight', 'engine-size' and 'highway-mpg' as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef32998f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(x_train[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac908ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_train = lr.predict(x_train[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']])\n",
    "yhat_train[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21efcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_test = lr.predict(x_test[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']])\n",
    "yhat_test[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956157ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db4c0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Title = 'Distribution  Plot of  Predicted Value Using Training Data vs Training Data Distribution (fig.1)'\n",
    "DistributionPlot(y_train, yhat_train, \"Actual Values (Train)\", \"Predicted Values (Train)\", Title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0ba321",
   "metadata": {},
   "source": [
    "So far, the model seems to be doing well in learning from the training dataset. When the model generates new values from the test data, we see the distribution of the predicted values is much different from the actual target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04179bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Title='Distribution  Plot of  Predicted Value Using Test Data vs Data Distribution of Test Data (fig.2)'\n",
    "DistributionPlot(y_test,yhat_test,\"Actual Values (Test)\",\"Predicted Values (Test)\",Title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05438ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558d4a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a60053",
   "metadata": {},
   "source": [
    "Comparing Figure 1 and Figure 2, it is evident that the distribution of the test data in Figure 1 is much better at fitting the data. This difference in Figure 2 is apparent in the range of 5000 to 15,000. This is where the shape of the distribution is extremely different."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b836d4",
   "metadata": {},
   "source": [
    "Let's see if polynomial regression also exhibits a drop in the prediction accuracy when analysing the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74a095d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535c9524",
   "metadata": {},
   "source": [
    "Overfitting occurs when the model fits the noise, but not the underlying process. Therefore, when testing your model using the test set, your model does not perform as well since it is modelling noise, not the underlying process that generated the relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368230f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use 55 percent of the data for training and the rest for testing:\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.45, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c952e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = PolynomialFeatures(degree=5)\n",
    "x_train_pr = pr.fit_transform(x_train[['horsepower']])\n",
    "x_test_pr = pr.fit_transform(x_test[['horsepower']])\n",
    "pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc55794e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a Linear Regression model \"poly\" and train it.\n",
    "poly = LinearRegression()\n",
    "poly.fit(x_train_pr, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4238e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = poly.predict(x_test_pr)\n",
    "yhat[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc57412c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the first five predicted values and compare it to the actual targets.\n",
    "print(\"Predicted values:\", yhat[0:4])\n",
    "print(\"True values:\", y_test[0:4].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7774eb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the function \"PollyPlot\" that we defined at the beginning of the lab to display the training data, testing data, and the predicted function.\n",
    "PollyPlot(x_train['horsepower'], x_test['horsepower'], y_train, y_test, poly,pr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09de7601",
   "metadata": {},
   "source": [
    "Figure 3: A polynomial regression model where red dots represent training data, green dots represent test data, and the blue line represents the model prediction.\n",
    "We see that the estimated function appears to track the data but around 200 horsepower, the function begins to diverge from the data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5171c9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R^2 of the training data:\n",
    "poly.score(x_train_pr, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b018f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#R^2 of the test data:\n",
    "poly.score(x_test_pr, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacec0f9",
   "metadata": {},
   "source": [
    "We see the R^2 for the training data is 0.5567 while the R^2 on the test data was -29.87. The lower the R^2, the worse the model. A negative R^2 is a sign of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e0166a",
   "metadata": {},
   "source": [
    "how the R^2 changes on the test data for different order polynomials and then plot the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cb21cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rsqu_test = []\n",
    "\n",
    "order = [1, 2, 3, 4]\n",
    "for n in order:\n",
    "    pr = PolynomialFeatures(degree=n)\n",
    "    \n",
    "    x_train_pr = pr.fit_transform(x_train[['horsepower']])\n",
    "    \n",
    "    x_test_pr = pr.fit_transform(x_test[['horsepower']])    \n",
    "    \n",
    "    lr.fit(x_train_pr, y_train)\n",
    "    \n",
    "    Rsqu_test.append(lr.score(x_test_pr, y_test))\n",
    "\n",
    "plt.plot(order, Rsqu_test)\n",
    "plt.xlabel('order')\n",
    "plt.ylabel('R^2')\n",
    "plt.title('R^2 Using Test Data')\n",
    "plt.text(3, 0.75, 'Maximum R^2 ')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6aeba9",
   "metadata": {},
   "source": [
    "We see the R^2 gradually increases until an order three polynomial is used. Then, the R^2 dramatically decreases at an order four polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9f480f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(order, test_data):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=test_data, random_state=0)\n",
    "    pr = PolynomialFeatures(degree=order)\n",
    "    x_train_pr = pr.fit_transform(x_train[['horsepower']])\n",
    "    x_test_pr = pr.fit_transform(x_test[['horsepower']])\n",
    "    poly = LinearRegression()\n",
    "    poly.fit(x_train_pr,y_train)\n",
    "    PollyPlot(x_train['horsepower'], x_test['horsepower'], y_train,y_test, poly, pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d430a9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(f, order=(0, 6, 1), test_data=(0.05, 0.95, 0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0a6959",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr1=PolynomialFeatures(degree=2)\n",
    "x_train_pr1=pr1.fit_transform(x_train[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']])\n",
    "\n",
    "x_test_pr1=pr1.fit_transform(x_test[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b9f5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_pr1.shape #there are now 15 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0f43a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly1=LinearRegression().fit(x_train_pr1,y_train)\n",
    "yhat_test1=poly1.predict(x_test_pr1)\n",
    "\n",
    "Title='Distribution  Plot of  Predicted Value Using Test Data vs Data Distribution of Test Data'\n",
    "\n",
    "DistributionPlot(y_test, yhat_test1, \"Actual Values (Test)\", \"Predicted Values (Test)\", Title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94326243",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The predicted value is higher than actual value for cars where the price $10,000 range, conversely the predicted price is lower than the price cost in the $30,000 to $40,000 range. As such the model is not as accurate in these ranges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a7138c",
   "metadata": {},
   "source": [
    "### Ridge Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c9bdb5",
   "metadata": {},
   "source": [
    "We will review Ridge Regression and see how the parameter alpha changes the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96181978",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr=PolynomialFeatures(degree=2)\n",
    "x_train_pr=pr.fit_transform(x_train[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg','normalized-losses','symboling']])\n",
    "x_test_pr=pr.fit_transform(x_test[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg','normalized-losses','symboling']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41793902",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63446c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "RigeModel=Ridge(alpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17926be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can fit the model using the method fit.\n",
    "RigeModel.fit(x_train_pr, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af33ad2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = RigeModel.predict(x_test_pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62aa4ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('predicted:', yhat[0:4])\n",
    "print('test set :', y_test[0:4].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5310c6",
   "metadata": {},
   "source": [
    "We select the value of alpha that minimizes the test error. To do so, we can use a for loop. We have also created a progress bar to see how many iterations we have completed so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf812254",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "Rsqu_test = []\n",
    "Rsqu_train = []\n",
    "dummy1 = []\n",
    "Alpha = 10 * np.array(range(0,1000))\n",
    "pbar = tqdm(Alpha)\n",
    "\n",
    "for alpha in pbar:\n",
    "    RigeModel = Ridge(alpha=alpha) \n",
    "    RigeModel.fit(x_train_pr, y_train)\n",
    "    test_score, train_score = RigeModel.score(x_test_pr, y_test), RigeModel.score(x_train_pr, y_train)\n",
    "    \n",
    "    pbar.set_postfix({\"Test Score\": test_score, \"Train Score\": train_score})\n",
    "\n",
    "    Rsqu_test.append(test_score)\n",
    "    Rsqu_train.append(train_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f74fb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can plot out the value of R^2 for different alphas:\n",
    "width = 12\n",
    "height = 10\n",
    "plt.figure(figsize=(width, height))\n",
    "\n",
    "plt.plot(Alpha,Rsqu_test, label='validation data  ')\n",
    "plt.plot(Alpha,Rsqu_train, 'r', label='training Data ')\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('R^2')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bef62b",
   "metadata": {},
   "source": [
    "The blue line represents the R^2 of the validation data, and the red line represents the R^2 of the training data. The x-axis represents the different values of Alpha."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0d0d02",
   "metadata": {},
   "source": [
    "Here the model is built and tested on the same data, so the training and test data are the same.\n",
    "\n",
    "The red line in Figure 4 represents the R^2 of the training data. As alpha increases the R^2 decreases. Therefore, as alpha increases, the model performs worse on the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fea9223",
   "metadata": {},
   "source": [
    "The blue line represents the R^2 on the validation data. As the value for alpha increases, the R^2 increases and converges at a point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aafcc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "RigeModel = Ridge(alpha=10) \n",
    "RigeModel.fit(x_train_pr, y_train)\n",
    "RigeModel.score(x_test_pr, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a163df1",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82954edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6955913c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a dictionary of parameter values:\n",
    "parameters1= [{'alpha': [0.001,0.1,1, 10, 100, 1000, 10000, 100000, 100000]}]\n",
    "parameters1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d8c08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge Regression\n",
    "RR=Ridge()\n",
    "RR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a049e0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a ridge grid search object:\n",
    "Grid1 = GridSearchCV(RR, parameters1,cv=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c69f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model:\n",
    "Grid1.fit(x_data[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']], y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2e1ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The object finds the best parameter values on the validation data. We can obtain the estimator with the best parameters and assign it to the variable BestRR as follows:\n",
    "BestRR=Grid1.best_estimator_\n",
    "BestRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8efb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now test our model on the test data:\n",
    "BestRR.score(x_test[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cec2725",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
